HOW TO HANDLE IMBALANCE DATA IN MACHINE LEARNING

    Classification problems are quite common in the machine learning world. 
    If you have already dealt with classification problems, you must have faced instances where one of the target class labels’ numbers of       observation is significantly lower than other class labels.
    Any usual approach to solving this kind of machine learning problem often yields inappropriate results.



What is imbalanced data?
    Imbalanced data refers to those types of datasets where the target class has an uneven distribution of observations, i.e one class label has a very high number of observations and the other has a very low number of observations. 



Problems with imbalanced data classification:
    the main problem with imbalanced dataset prediction is how accurately are we actually predicting both majority and minority class? 



Approach to deal with the imbalanced dataset problem:

    In rare cases like fraud detection or disease prediction, it is vital to identify the minority classes correctly.
    So model shouldn't be biased to detect only the majority cls but should give equal weight or importance towards the minority class too.     
    Here are some of the few techniques which can deal with this problem. 
    There is no right method or wrong method in this, different techniques work well with different problems.

1. Use the right evaluation metrics
 
        If accuracy is used to measure the goodness of a model, a model which classifies all testing samples into “0” will have an excellent           accuracy (99.8%), but obviously, this model won’t provide any valuable information for us.

        In this case, other alternative evaluation metrics can be applied such as:

        Precision/Specificity: how many selected instances are relevant.
        Recall/Sensitivity: how many relevant instances are selected.
        F1 score: harmonic mean of precision and recall.
        MCC: correlation coefficient between the observed and predicted binary classifications.
        AUC: relation between true-positive rate and false positive rate.
        
2. Resample the training set
 
        Apart from using different evaluation criteria, one can also work on getting different dataset. Two approaches to make a balanced           dataset out of an imbalanced one are under-sampling and over-sampling.

        2.1. Under-sampling

            Under-sampling balances the dataset by reducing the size of the abundant class. This method is used when quantity of data is                 sufficient(less). 

            By keeping all samples in the rare class and randomly selecting an equal number of samples in the abundant class, a                         balanced new dataset can be retrieved for further modelling.

            Example: 
            yes   No     =>   yes   No
            900   100         100   100
            It ll eliminate some of the data points to make sure that whatever unique categories in output feature must be in same ratio.
            We have a data loss over here, 800 data points are lost so it is not advisible for real-world applications.

        2.2. Over-sampling

            On the contrary, oversampling is used when the quantity of data is insufficient(Huge). It tries to balance dataset by increasing             the size of rare samples. 

            Rather than getting rid of abundant samples, new rare samples are generated by using e.g. repetition, bootstrapping or SMOTE                 (Synthetic Minority Over-Sampling Technique) [1].

            Example:
            yes   No      =>   yes  No
            900   100          900  900
            It ll add some extra data points to No node which is highly advisible 

3. Ensemble different resampled datasets
 
        The easiest way to successfully generalize a model is by using more data. 
        
        The problem is out-of-the-box classifiers like logistic regression or random forest tend to generalize by discarding the rare class.
        
        One best practice is building n models that use all the samples of the rare class and n-differing samples of the abundant class.  
        
        Given that you want to ensemble 10 models, you would keep e.g. the 1.000 cases of the rare class and randomly sample 10.000 cases of         the abundant class. 
        
        Then you just split the 10.000 cases in 10 chunks and train 10 different models.
        
        This approach is simple and perfectly horizontally scalable if you have a lot of data, since you can just train and run your models         on different cluster nodes. 
        
        Ensemble models also tend to generalize better, which makes this approach easy to handle.


        
        
        
        
        
        
        
        
        
